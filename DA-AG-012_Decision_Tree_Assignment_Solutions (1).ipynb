{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366b158d",
   "metadata": {},
   "source": [
    "# Decision Tree | Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fceb0",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**What is a Decision Tree, and how does it work in the context of classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fae40",
   "metadata": {},
   "source": [
    "### Answer\n",
    "A **Decision Tree** is a supervised learning model that predicts a target by learning simple decision rules inferred from data features.  \n",
    "For **classification**, the tree recursively partitions the feature space into regions that are (ideally) pure in terms of class labels:\n",
    "\n",
    "1. **Root node**: Start with all training data.  \n",
    "2. **Split**: Choose a feature and threshold that best separates classes (e.g., by maximizing information gain or minimizing Gini impurity).  \n",
    "3. **Recursion**: Repeat splitting on each child node until a stopping criterion is met (e.g., max depth, minimum samples, or pure leaf).  \n",
    "4. **Prediction**: For a new sample, traverse the tree from root to a **leaf** following the decision rules; the predicted class is the majority class at that leaf.\n",
    "\n",
    "Trees are **interpretable** (the path is a human-readable set of if/else rules), handle mixed feature types, and require little preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd52c94",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35997c",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Let a node have class proportions \\(p_1, p_2, \\dots, p_K\\).\n",
    "\n",
    "- **Gini Impurity**: \\( G = 1 - \\sum_{k=1}^K p_k^2 \\). Lower Gini means purer nodes.  \n",
    "- **Entropy**: \\( H = -\\sum_{k=1}^K p_k \\log_2 p_k \\). Entropy is 0 when a node is pure and maximal when classes are uniform.\n",
    "\n",
    "**Impact on splits**: During training, the algorithm evaluates candidate splits and chooses the one that **reduces impurity** the most (i.e., maximizes **information gain** for entropy or minimizes Gini). In practice, Gini and Entropy often give similar trees; Gini is a bit faster to compute, while entropy has an information-theoretic interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53340438",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "**What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80fb3a",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **Pre-pruning (early stopping)**: Limit tree growth during training using constraints like `max_depth`, `min_samples_split`, `min_samples_leaf`, or `max_leaf_nodes`.\n",
    "\n",
    "  - *Advantage*: Faster training and smaller, more interpretable trees that generalize better when data is noisy.\n",
    "\n",
    "- **Post-pruning (cost-complexity pruning)**: Grow a large tree first, then prune back using a complexity penalty (e.g., `ccp_alpha` in scikit-learn) based on validation performance.\n",
    "\n",
    "  - *Advantage*: Can discover a strong large tree first and then optimally simplify it to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b9dd4",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c16a83",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**Information Gain (IG)** is the **reduction in impurity** (often entropy) achieved by a split. If \\(H(\\text{parent})\\) is the impurity of a node and the split creates children with impurities \\(H(\\text{left}), H(\\text{right})\\) and proportions \\(w_l, w_r\\), then:\n",
    "\\[ IG = H(\\text{parent}) - \\big(w_l H(\\text{left}) + w_r H(\\text{right})\\big). \\]\n",
    "\n",
    "A split with higher IG yields children that are **purer**, improving class separability and usually predictive performance. The tree chooses the split (feature + threshold) that **maximizes** IG (or equivalently minimizes weighted child impurity).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac5d96",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "**What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2faa1d",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**Applications**: credit approval, fraud detection, medical diagnosis, churn prediction, risk scoring, quality control, and simple rule-based decision support.\n",
    "\n",
    "**Advantages**: interpretable rules, little preprocessing, handles nonlinearity and interactions, works with mixed data types, robust to monotonic transformations.\n",
    "\n",
    "**Limitations**: high variance (overfitting) if unpruned, axis-aligned splits can be myopic, small changes in data can change the tree; ensembles (Random Forests, Gradient Boosting) often perform better but lose some interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516cc05",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "**Load the Iris Dataset; train a Decision Tree Classifier using the Gini criterion; print the model’s accuracy and feature importances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aff26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Gini): 0.8947\n",
      "\n",
      "Feature Importances:\n",
      "  sepal length (cm)         0.0134\n",
      "  sepal width (cm)          0.0201\n",
      "  petal length (cm)         0.9199\n",
      "  petal width (cm)          0.0466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Train classifier with Gini\n",
    "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy (Gini): {acc:.4f}\\n\")\n",
    "print(\"Feature Importances:\")\n",
    "for name, imp in zip(feature_names, clf_gini.feature_importances_):\n",
    "    print(f\"  {name:25s} {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b18ae",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "**Train a Decision Tree Classifier with `max_depth=3` and compare its accuracy to a fully-grown tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7d3d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (max_depth=3): 0.8947\n",
      "Accuracy (fully-grown): 0.8947\n"
     ]
    }
   ],
   "source": [
    "# Constrained depth tree\n",
    "clf_depth3 = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "acc_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
    "\n",
    "# Fully-grown (use previous clf_gini with no depth limit)\n",
    "acc_full = acc  # from Q6\n",
    "\n",
    "print(f\"Accuracy (max_depth=3): {acc_depth3:.4f}\")\n",
    "print(f\"Accuracy (fully-grown): {acc_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72f461",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "**(Offline-friendly)** Train a Decision Tree Regressor on a built-in regression dataset and report MSE and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3501c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5941.7027\n",
      "\n",
      "Feature Importances:\n",
      "  age                       0.0501\n",
      "  sex                       0.0194\n",
      "  bmi                       0.4145\n",
      "  bp                        0.0777\n",
      "  s1                        0.0749\n",
      "  s2                        0.0595\n",
      "  s3                        0.0572\n",
      "  s4                        0.0314\n",
      "  s5                        0.1397\n",
      "  s6                        0.0756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset (offline, bundled with scikit-learn)\n",
    "dia = load_diabetes()\n",
    "Xr, yr = dia.data, dia.target\n",
    "feature_names_reg = dia.feature_names\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train regressor\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "dtr.fit(Xr_train, yr_train)\n",
    "\n",
    "# Evaluate\n",
    "pred = dtr.predict(Xr_test)\n",
    "mse = mean_squared_error(yr_test, pred)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\\n\")\n",
    "print(\"Feature Importances:\")\n",
    "for name, imp in zip(feature_names_reg, dtr.feature_importances_):\n",
    "    print(f\"  {name:25s} {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a614ad9",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "**Tune the Decision Tree’s `max_depth` and `min_samples_split` using GridSearchCV; print the best parameters and accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0c37b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Test Accuracy with Best Model: 0.8947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                    param_grid=param_grid,\n",
    "                    cv=5,\n",
    "                    n_jobs=None)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best = grid.best_estimator_\n",
    "best_acc = accuracy_score(y_test, best.predict(X_test))\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(f\"Test Accuracy with Best Model: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17147709",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "**End-to-end plan for disease prediction with mixed data and missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a703f4",
   "metadata": {},
   "source": [
    "### Answer (Step-by-step)\n",
    "1. **Exploration & Target Definition**: Understand label prevalence, feature types (numeric, categorical, ordinal), leakage, and business KPIs (e.g., recall@precision, AUROC).\n",
    "\n",
    "2. **Handling Missing Values**: Use imputers: median for skewed numeric, mean for roughly normal numeric, most-frequent for low-cardinality categorical. Consider **iterative imputation** or domain-driven defaults where appropriate. Keep **missingness indicators** when it correlates with the label.\n",
    "\n",
    "3. **Encoding Categorical Features**: For trees, minimal encoding is needed (label encoding works). If using one-hot, ensure reasonable dimensionality (group rare levels, or use target encoding with leakage-safe CV).\n",
    "\n",
    "4. **Train/Validation Split**: Stratified split (or time-based split if temporal). Keep a hold-out test set. Address class imbalance with class weights (e.g., `class_weight='balanced'`) or resampling.\n",
    "\n",
    "5. **Model Training (Decision Tree)**: Start with constraints: `max_depth`, `min_samples_split`, `min_samples_leaf`. Use pipelines to bundle imputers + encoders + classifier.\n",
    "\n",
    "6. **Hyperparameter Tuning**: Grid/random search over depth, leaf size, split criteria, and `ccp_alpha` (cost-complexity pruning). Optimize for a metric aligned to business goals (e.g., **recall** to reduce missed positives) and include calibration if probabilistic thresholds matter.\n",
    "\n",
    "7. **Evaluation**: Report accuracy, precision/recall/F1, ROC-AUC, PR-AUC, confusion matrix, and calibration curve. Perform cross-validation. Check subgroup performance for fairness.\n",
    "\n",
    "8. **Business Value**: The model prioritizes patients for further testing or early intervention, optimizing clinician time and cost while improving patient outcomes. Use decision thresholds aligned with capacity and risk tolerance, monitor drift, and retrain on new data.\n",
    "\n",
    "9. **Deployment & Monitoring**: Serve via an API or batch scoring; log predictions, feature distributions, and outcomes; set up alerts for performance decay and data drift.\n",
    "\n",
    "\n",
    "\n",
    "**Note**: Tree-based ensembles (Random Forests/Gradient Boosting) typically outperform a single tree while remaining relatively interpretable (feature importances, SHAP values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
